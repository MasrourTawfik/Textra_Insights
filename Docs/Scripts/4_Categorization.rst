Categorization
===============
*PCG (Plan comptable général)*

Now let's implement the second step of our pipeline: **Categorization**. As explained in the **Introduction** section, we aim to categorize invoices into PCG accounts. 
This can be achieved by using an **Advanced RAG** pipeline, utilizing definitions extracted from a PCG PDF file, which can be found online.

You can find the PCG PDF file `here <https://github.com/MasrourTawfik/Textra_Insights/tree/main/Files>`_.

Bellow is the general process of the Categorization pipeline :

.. figure:: /Docs/Images/4_Categorization/Image1.png
   :width: 80%
   :align: center
   :alt: Categorization
   :name: Pipeline

**1. Descriptor** 

The **Descriptor** is a Large Language Model (LLM) responsible for: 

- Generating a simple analysis of the invoice.  
- Identifying the nature of the products/services.  
- Guessing the **Debit Account**.  

**2. RAG (Retrieval-Augmented Generation)**  

The **RAG** contains definitions of **PCG accounts**.  

- The **Retriever** will fetch the candidate accounts.  
- The **Refiner** will determine the appropriate **Debit Account** based on the **Prompt**.

- The **Prompt** holds all the analysis generated by the **Descriptor**.
- The possible **Credit Account** and **VAT Account** can be defined based on the **PCG file**.

1.Data Preparation
-------------------

If you consult the previous **PCG file**, you notice that is not editable, it's a bunch of scanned images.
To be able to use these definitions we need to **digitize** and **clean** them.

.. note::
   
   - From a quick search on the internet about **Payment Invoices** you find the most relevant Classes is the **PCG file** are :
    
     - *Classe 2  : Comptes d'actif immobilisé (page 18-27 in PCG file)*

        - *21 IMMOBILISATIONS EN NON-VALEURS*
        - *22 IMMOBILISATIONS INCORPORELLES*
        - *23 IMMOBILISATIONS CORPORELLES*
   
     - *Classe 6 : Comptes de charges (page 85-101 in PCG file)*

        - *61 CHARGES D'EXPLOITATION*
   
   - So we focused only on these two classes and their accounts.
   - We extract only intreasted pages from **PCG General** to **PCG file**.

1.1 Digitization
+++++++++++++++++++++

We used **Marker Engine** implemented in the top of **Surya OCR** to convert our **PCG file** into **Markdown** format.

The official Github repository of **Marker** can be found `here <https://github.com/VikParuchuri/marker>`_.

1.2 Cleaning
+++++++++++++

To clean the result Markdown file we need to remove :

- Tables (because they are not usefull for our purpose)
- Classes and Sub Classes (because we are intreasted only in the Accounts)

After this the cleaned markdown file can be converted into **json** then to **csv** format so it will be easy to work with.

The final desired output locks like this :

.. figure:: /Docs/Images/4_Categorization/Image2.png
   :width: 100%
   :align: center
   :alt: Categorization
   :name: Pipeline

- You can find the CSV file `here <https://github.com/MasrourTawfik/Textra_Insights/tree/main/Files>`_.


.. raw:: html

   <a href="https://colab.research.google.com/github/MasrourTawfik/Textra_Insights/blob/main/Notebooks/Data_Preparation.ipynb" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>


2.Raw Definitions
------------------

To understand why we need to refine the definitions extracted from the PCG file, we take here an example of a invoice.

- We run an PaddleOCR on a **Lydec** invoice.
- The resulting text passs to **Llama3.1-8b** to generate a simple analysis (prompt).
- Then we took 3 condidates definitions one of them is the right one. (account *6125*)
- We embedd the definitions with the prompt and we mesure the similarities.

.. figure:: /Docs/Images/4_Categorization/Image4.png
   :width: 100%
   :align: center
   :alt: Categorization
   :name: Pipeline

Here is the result of the similarities :

.. figure:: /Docs/Images/4_Categorization/Image3.png
   :width: 100%
   :align: center
   :alt: Categorization
   :name: Pipeline

You notice that the similarities are close to each other, indicating a not **reliable** retrieval process.

3.Different Approaches
------------------------

Enhancing this retrieval process can be done by:

- Refining the definitions with **keywords** and **examples**.
- Benchmarking multiple embedding models and choosing the best one.
- Fine-tuning the embedding model for our use case.

4.Evaluation Strategy
-----------------------

To test these approaches we need first to an evaluation dataset.

4.1 Grounding Truth
-----------------------









